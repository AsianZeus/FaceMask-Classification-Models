{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MCyAOA4MFHGm"
      },
      "outputs": [],
      "source": [
        "!rm -r sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jre3sA_kJFkF",
        "outputId": "50542b1b-24ce-497e-9695-933281a4fe5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yLHrLwH0FSmA",
        "outputId": "752da2ca-f9a6-47d4-8037-6a1f8617e473"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# ! pip install datasets transformers\n",
        "! pip install --upgrade --force-reinstall git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnwOAAaARy6p",
        "outputId": "9661a03a-071d-4a69-ddd1-359bb6d64139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MaskDetection\n"
          ]
        }
      ],
      "source": [
        "%cd MaskDetection/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq62cdJ8FnBG",
        "outputId": "8d692af6-d5f9-483d-b8a7-ce36fd0562ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading feature extractor configuration file https://huggingface.co/facebook/convnext-tiny-224/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/9680dfbfa38989323a5ae0b86951c5860c092e3aae6dc354483c075fee48c1a1.37be7274d6b5860aee104bb1fbaeb0722fec3850a85bb2557ae9491f17f89433\n",
            "Feature extractor ConvNextFeatureExtractor {\n",
            "  \"crop_pct\": 0.875,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n",
            "  \"image_mean\": [\n",
            "    0.485,\n",
            "    0.456,\n",
            "    0.406\n",
            "  ],\n",
            "  \"image_std\": [\n",
            "    0.229,\n",
            "    0.224,\n",
            "    0.225\n",
            "  ],\n",
            "  \"resample\": 3,\n",
            "  \"size\": 224\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader\n",
        ")\n",
        "\n",
        "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\n",
        "\n",
        "model_name_or_path = 'facebook/convnext-tiny-224'\n",
        "feature_extractor = ConvNextFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "\n",
        "class FeatureExtractor(object):\n",
        "    def __call__(self, image, target):\n",
        "        sample = feature_extractor(image, return_tensors='pt')\n",
        "        sample[\"labels\"] = target\n",
        "        return sample\n",
        "\n",
        "class MaskDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        "        image = io.imread(img_path)\n",
        "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
        "        \n",
        "        data = self.transform(image,y_label)\n",
        "\n",
        "        return data\n",
        "\n",
        "train_ds = MaskDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    root_dir=\"\",\n",
        "    # transform=transforms.ToTensor(),\n",
        "    transform=FeatureExtractor(),\n",
        ")\n",
        "\n",
        "test_ds = MaskDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    root_dir=\"\",\n",
        "    # transform=transforms.ToTensor(),\n",
        "    transform=FeatureExtractor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO4OCrCnIGwq",
        "outputId": "0d5c85c2-e03c-4cd5-c6d7-c019a14ff6c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds.__getitem__(0)['pixel_values'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "669332bcc24146f087e444c7ffb6a275",
            "3112efa66ba84643ab73967751947782",
            "cc3592ded0ca4bf7922a53bd9864084a",
            "6a8d9b27cde847e69a573d11768561ec",
            "a4c936ca3fa24003aeb2a9b98e56a225",
            "1200bc9fee3c4f928fbb0bbfc7f3670b",
            "a8b414f3b1644242afb4f6c520ab01fe",
            "1b287c59fb9a4de5b0e13fad5c830a63",
            "7700bc3da13a46a7910176dc4a4729e0",
            "0549ecb05de5472cbf98f24865711927",
            "2f799f3a9de54e639144957a17be1368"
          ]
        },
        "id": "O-Qu1FRxIGul",
        "outputId": "44d168ca-6e3f-43fc-a729-d06b29b20318"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "669332bcc24146f087e444c7ffb6a275",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9ByNTzfIGsg",
        "outputId": "f3e6d410-1f02-4f99-d4be-75c731b2dcb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/facebook/convnext-tiny-224/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/741b590dfc3966dd77875cd496d6f9097645132a8e8106ab873b7d130da4cab9.a98bcd3c1d885a60ccf8ce2be18a604366824aabd552be8f43a9f2206d339247\n",
            "Model config ConvNextConfig {\n",
            "  \"architectures\": [\n",
            "    \"ConvNextForImageClassification\"\n",
            "  ],\n",
            "  \"depths\": [\n",
            "    3,\n",
            "    3,\n",
            "    9,\n",
            "    3\n",
            "  ],\n",
            "  \"drop_path_rate\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_sizes\": [\n",
            "    96,\n",
            "    192,\n",
            "    384,\n",
            "    768\n",
            "  ],\n",
            "  \"id2label\": {\n",
            "    \"0\": \"No Mask\",\n",
            "    \"1\": \"Mask\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"Mask\": \"1\",\n",
            "    \"No Mask\": \"0\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layer_scale_init_value\": 1e-06,\n",
            "  \"model_type\": \"convnext\",\n",
            "  \"num_channels\": 3,\n",
            "  \"num_stages\": 4,\n",
            "  \"patch_size\": 4,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0.dev0\"\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/facebook/convnext-tiny-224/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f6cde2591c44e990d97372da4184ff364f9270bd3e051a10e9ebc7f077b2c276.6164ec88009a0d416d211404f7a5feaafb35327b50613e99aec53791a3daabdc\n",
            "All model checkpoint weights were used when initializing ConvNextForImageClassification.\n",
            "\n",
            "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-tiny-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "labels = [\"No Mask\",\"Mask\"]\n",
        "\n",
        "model = ConvNextForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    ignore_mismatched_sizes=True,\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr95lWdWIGqZ",
        "outputId": "3189f96f-2de7-4619-cfcf-d4339a39fe1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./convnext-tiny-mask-finetuned\",\n",
        "  per_device_train_batch_size=64,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=4,\n",
        "  save_total_limit = 4, # Only last 4 models are saved. Older ones are deleted.\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pQDGsAPiIGoW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'][0] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w97XyHiIGlw",
        "outputId": "f2e4c6c2-5268-47c2-81ed-7901d1d283a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=feature_extractor,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FRyjG3JWIGjr",
        "outputId": "cc728b65-ca5c-483b-aa35-5d267a0890a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 14428\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 904\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [800/904 1:08:16 < 08:53, 0.19 it/s, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>0.023499</td>\n",
              "      <td>0.994438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>0.993604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.017131</td>\n",
              "      <td>0.994716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.018141</td>\n",
              "      <td>0.995829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.996107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.010200</td>\n",
              "      <td>0.016863</td>\n",
              "      <td>0.994994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.019649</td>\n",
              "      <td>0.996107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.017847</td>\n",
              "      <td>0.995551</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-100\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-100/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-100/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-100/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-200\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-200/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-200/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-200/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-300\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-300/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-300/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-300/preprocessor_config.json\n",
            "Deleting older checkpoint [convnext-tiny-mask-finetuned/checkpoint-400] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-400\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-400/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-400/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-400/preprocessor_config.json\n",
            "Deleting older checkpoint [convnext-tiny-mask-finetuned/checkpoint-600] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-500\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-500/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-500/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-500/preprocessor_config.json\n",
            "Deleting older checkpoint [convnext-tiny-mask-finetuned/checkpoint-100] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-600\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-600/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-600/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-600/preprocessor_config.json\n",
            "Deleting older checkpoint [convnext-tiny-mask-finetuned/checkpoint-200] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-700\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-700/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-700/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-700/preprocessor_config.json\n",
            "Deleting older checkpoint [convnext-tiny-mask-finetuned/checkpoint-300] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned/checkpoint-800\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/checkpoint-800/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/checkpoint-800/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/checkpoint-800/preprocessor_config.json\n",
            "Deleting older checkpoint [convnext-tiny-mask-finetuned/checkpoint-400] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./convnext-tiny-mask-finetuned/checkpoint-500 (score: 0.015090291388332844).\n",
            "Saving model checkpoint to ./convnext-tiny-mask-finetuned\n",
            "Configuration saved in ./convnext-tiny-mask-finetuned/config.json\n",
            "Model weights saved in ./convnext-tiny-mask-finetuned/pytorch_model.bin\n",
            "Feature extractor saved in ./convnext-tiny-mask-finetuned/preprocessor_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         3.54\n",
            "  total_flos               = 1195651761GF\n",
            "  train_loss               =       0.0079\n",
            "  train_runtime            =   1:08:20.25\n",
            "  train_samples_per_second =       14.075\n",
            "  train_steps_per_second   =         0.22\n"
          ]
        }
      ],
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "vWhlNQHJIGhS",
        "outputId": "42797a1f-c3c1-45cb-959b-1d617df039a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 3596\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [450/450 01:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =       3.54\n",
            "  eval_accuracy           =     0.9961\n",
            "  eval_loss               =     0.0151\n",
            "  eval_runtime            = 0:01:23.47\n",
            "  eval_samples_per_second =     43.079\n",
            "  eval_steps_per_second   =      5.391\n"
          ]
        }
      ],
      "source": [
        "metrics = trainer.evaluate(test_ds)\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ConvNeXt-Mask-FineTuned.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0549ecb05de5472cbf98f24865711927": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1200bc9fee3c4f928fbb0bbfc7f3670b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b287c59fb9a4de5b0e13fad5c830a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f799f3a9de54e639144957a17be1368": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3112efa66ba84643ab73967751947782": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669332bcc24146f087e444c7ffb6a275": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc3592ded0ca4bf7922a53bd9864084a",
              "IPY_MODEL_6a8d9b27cde847e69a573d11768561ec",
              "IPY_MODEL_a4c936ca3fa24003aeb2a9b98e56a225"
            ],
            "layout": "IPY_MODEL_3112efa66ba84643ab73967751947782"
          }
        },
        "6a8d9b27cde847e69a573d11768561ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7700bc3da13a46a7910176dc4a4729e0",
            "max": 1411,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b287c59fb9a4de5b0e13fad5c830a63",
            "value": 1411
          }
        },
        "7700bc3da13a46a7910176dc4a4729e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c936ca3fa24003aeb2a9b98e56a225": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f799f3a9de54e639144957a17be1368",
            "placeholder": "​",
            "style": "IPY_MODEL_0549ecb05de5472cbf98f24865711927",
            "value": " 3.19k/? [00:00&lt;00:00, 58.9kB/s]"
          }
        },
        "a8b414f3b1644242afb4f6c520ab01fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc3592ded0ca4bf7922a53bd9864084a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8b414f3b1644242afb4f6c520ab01fe",
            "placeholder": "​",
            "style": "IPY_MODEL_1200bc9fee3c4f928fbb0bbfc7f3670b",
            "value": "Downloading: "
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
